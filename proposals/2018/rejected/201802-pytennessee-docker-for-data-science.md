# Data Science Workflows using Docker Containers

## Audience Level

Intermediate

## Brief Description

Jupyter notebooks make it easy to create reproducible workflows that can be distributed across groups and organizations. This is a simple process provided our end-users have access to the data along with a compatible Python environment. Learn how to use Docker to package a shareable image containing the libraries, code, and data required to reproduce every calculation.

## Detailed Abstract

Containerization technologies such as Docker enable software to run across various computing environments. Data Science requires auditable workflows where we can easily share and reproduce results. Docker is a useful tool that we can use to package libraries, code, and data into a single image.

This talk will cover the basics of Docker; discuss how containers fit into various Data Science workflows; and provide a quick-start guide that can be used as a template to create a shareable Docker image!

Learn how to leverage the power of Docker without having to worry about the underlying details of the technology. Although this session is geared towards data scientists, the underlying concepts have many use cases (come find me after to discuss).

## Additional Notes

* [Link to slides](http://bit.ly/docker-for-data-science)
* [Previous talks](https://github.com/alysivji/talks)

For this talk, I assume no knowledge of Docker. We will build up from first principles and I alternate between slides and the terminal to guide the audience thru my workflow and thought process.
