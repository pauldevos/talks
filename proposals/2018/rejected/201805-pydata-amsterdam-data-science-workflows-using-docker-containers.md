# Data Science Workflows using Docker Containers

## Audience Level

Intermediate

## Brief Summary

Jupyter notebooks make it easy to create reproducible workflows that can be distributed across groups and organizations. This is a simple process provided our end-users have access to the data along with a compatible Python environment. Learn how to use Docker to package a shareable image containing the libraries, code, and data required to reproduce every calculation.

## Description

Containerization technologies such as Docker enable software to run across various computing environments. Data Science requires auditable workflows where we can easily share and reproduce results. Docker is a useful tool that we can use to package libraries, code, and data into a single image.

This talk will cover the basics of Docker; discuss how containers fit into various Data Science workflows; and provide a quick-start guide that can be used as a template to create a shareable Docker image!

Learn how to leverage the power of Docker without having to worry about the underlying details of the technology.

## Additional Notes

Will be traveling internationally (from Chicago) for this talk.

* [Link to slides](http://bit.ly/docker-for-data-science)
* [Previous talks](https://github.com/alysivji/talks)

For this talk, I assume no knowledge of Docker. We will build up from first principles and I alternate between slides and the terminal to guide the audience thru my workflow and thought process.

## Official Keywords

- [x] Best Practices
- [x] Devops: Pipelines, Deployment, Scalability, Packaging
- [x] Jupyter / Notebooks
- [x] Reproducible Science
